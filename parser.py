#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""–ü–∞—Ä—Å–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å —Å–∞–π—Ç–æ–≤ –≥–∞–∑–µ—Ç—ã"""

import logging
import re
import time
import xml.etree.ElementTree as ET
from urllib.parse import quote

import requests
from bs4 import BeautifulSoup

from config import (
    SITE_URL,
    OLD_SITE_URL,
    USER_AGENT,
    REQUEST_TIMEOUT,
    ARTICLES_LIMIT_SEARCH,
    OKOLICA_HTML_PAGES,
)

logger = logging.getLogger(__name__)


def _make_request(url: str, params: dict = None, retries: int = 2) -> requests.Response:
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP-–∑–∞–ø—Ä–æ—Å–∞ —Å –æ–±—â–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏ –ø–æ–≤—Ç–æ—Ä–æ–º –ø—Ä–∏ 5xx."""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
    }
    last_error = None
    for attempt in range(retries + 1):
        try:
            resp = requests.get(
                url,
                params=params,
                headers=headers,
                timeout=REQUEST_TIMEOUT,
            )
            if resp.status_code == 503 and attempt < retries:
                time.sleep(1.5 * (attempt + 1))
                continue
            return resp
        except requests.RequestException as e:
            last_error = e
            if attempt < retries:
                time.sleep(1.0)
    raise last_error


def get_latest_articles(limit: int = 10) -> list[dict]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π —Å sibokolica.ru."""
    try:
        response = _make_request(SITE_URL)
        response.encoding = "utf-8"
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        articles = []

        for h2 in soup.find_all("h2"):
            link = h2.find("a")
            if not link or not link.get("href") or ".html" not in link.get("href", ""):
                continue

            href = link.get("href", "")
            if not href.startswith("http"):
                href = SITE_URL + (href if href.startswith("/") else "/" + href)

            title = h2.get_text(strip=True)
            if not title:
                continue

            summary = _extract_summary(h2, title, max_len=200)

            articles.append({"title": title, "url": href, "summary": summary})

            if len(articles) >= limit:
                break

        return articles

    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ sibokolica.ru: %s", e)
        return []


def _extract_summary(anchor_element, exclude_text: str, max_len: int = 200) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤."""
    parent = anchor_element.parent
    for _ in range(5):
        if not parent:
            break
        for elem in parent.find_all(["p", "div"]):
            txt = elem.get_text(strip=True)
            if (
                txt
                and txt != exclude_text
                and 30 < len(txt) < 300
                and not txt.startswith("http")
                and elem != anchor_element
            ):
                return txt[:max_len] + ("..." if len(txt) > max_len else "")
        parent = parent.parent
    return ""


def _elem_text(el) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ XML-—ç–ª–µ–º–µ–Ω—Ç–∞ (–≤–∫–ª—é—á–∞—è CDATA)."""
    if el is None:
        return ""
    return "".join(el.itertext()).strip()


def _fetch_okolica_rss() -> list[dict]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ RSS okolica.net.
    RSS —Å–æ–¥–µ—Ä–∂–∏—Ç title, description, fulltext ‚Äî –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞.
    """
    try:
        url = f"{OLD_SITE_URL}/news/rss.xml"
        response = _make_request(url)
        response.raise_for_status()
        try:
            text = response.content.decode("cp1251")
        except UnicodeDecodeError:
            text = response.content.decode("utf-8", errors="replace")
        root = ET.fromstring(text)
        items = root.findall(".//item")

        articles = []
        for item in items:
            link_el = item.find("link")
            if link_el is None or not link_el.text or "/news/" not in link_el.text:
                continue

            url_str = link_el.text.strip()
            if not url_str.startswith("http"):
                url_str = f"{OLD_SITE_URL}{url_str}" if url_str.startswith("/") else url_str
            url_str = url_str.replace("http://", "https://", 1)

            title_el = item.find("title")
            title = _elem_text(title_el)
            if not title:
                continue

            desc_el = item.find("description")
            summary = _elem_text(desc_el)[:200] if desc_el is not None else ""

            full_el = item.find("fulltext")
            fulltext = _elem_text(full_el) if full_el is not None else ""

            articles.append({
                "title": title,
                "url": url_str,
                "summary": summary,
                "_fulltext": fulltext,
            })

        return articles

    except ET.ParseError as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS okolica.net: %s", e)
        return []
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RSS okolica.net: %s", e)
        return []


def _fetch_okolica_html(max_pages: int = None) -> list[dict]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü HTML okolica.net (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø—É–ª–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞).
    –°–∞–π—Ç –≤ cp1251, –ø–∞—Ä—Å–∏–º —Å—Å—ã–ª–∫–∏ –∏–∑ –ª–µ–Ω—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π.
    """
    max_pages = max_pages or OKOLICA_HTML_PAGES
    all_articles = []
    seen_urls = set()

    for page in range(1, max_pages + 1):
        try:
            url = f"{OLD_SITE_URL}/news/" + (f"?page={page}" if page > 1 else "")
            response = _make_request(url)
            response.raise_for_status()
            try:
                text = response.content.decode("cp1251")
            except UnicodeDecodeError:
                text = response.content.decode("utf-8", errors="replace")
            soup = BeautifulSoup(text, "html.parser")

            for a in soup.find_all("a", href=True):
                href = a.get("href", "")
                if "/news/" not in href or ".html" not in href or "rss" in href:
                    continue
                if "/top.html" in href or "/last.html" in href:
                    continue
                if not re.search(r"/news/[^/]+/\d+\.html", href):
                    continue
                if href in seen_urls:
                    continue

                full_url = OLD_SITE_URL + href if not href.startswith("http") else href
                title = a.get_text(strip=True)
                title = re.sub(r"\s*\[\.\.\.\]\s*$", "", title)
                if not title or len(title) < 5:
                    continue

                seen_urls.add(href)
                full_url_https = full_url.replace("http://", "https://", 1)
                all_articles.append({
                    "title": title,
                    "url": full_url_https,
                    "summary": "",
                    "_fulltext": "",
                })

        except Exception as e:
            logger.warning("–û—à–∏–±–∫–∞ HTML okolica.net page %s: %s", page, e)
            break

    return all_articles


def _merge_okolica_sources(rss_articles: list[dict], html_articles: list[dict]) -> list[dict]:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç RSS –∏ HTML: RSS –¥–∞—ë—Ç fulltext/summary (~10 —Å—Ç–∞—Ç–µ–π), HTML ‚Äî —à–∏—Ä–æ–∫–∏–π –æ—Ö–≤–∞—Ç (—Å–æ—Ç–Ω–∏).
    –ü—Ä–∏ –¥—É–±–ª–∏–∫–∞—Ç–µ –ø–æ URL –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É RSS.
    """
    by_url: dict[str, dict] = {}
    for a in rss_articles:
        url = a["url"].replace("http://", "https://", 1).rstrip("/")
        by_url[url] = a
    for a in html_articles:
        url = a["url"].replace("http://", "https://", 1).rstrip("/")
        if url not in by_url:
            by_url[url] = a
    return list(by_url.values())


def search_okolica_only(query: str, limit: int = None) -> list[dict]:
    """
    –ü–æ–∏—Å–∫ –Ω–∞ okolica.net –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
    –í—Å–µ–≥–¥–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç RSS (title, description, fulltext ~10 —à—Ç.) –∏ HTML (–º–Ω–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü).
    """
    limit = limit or ARTICLES_LIMIT_SEARCH

    try:
        rss_articles = _fetch_okolica_rss()
        html_articles = _fetch_okolica_html()
        articles = _merge_okolica_sources(rss_articles, html_articles)

        if not articles:
            return []

        query_words = [w.strip().lower() for w in query.split() if w.strip()]
        if not query_words:
            return [
                {"title": a["title"], "url": a["url"], "summary": a.get("summary", "")}
                for a in articles[:limit]
            ]

        matched = []
        for a in articles:
            searchable = f"{a['title']} {a.get('summary', '')} {a.get('_fulltext', '')}".lower()
            if all(w in searchable for w in query_words):
                matched.append({"title": a["title"], "url": a["url"], "summary": a.get("summary", "")})
                if len(matched) >= limit:
                    break

        if not matched:
            for a in articles:
                searchable = f"{a['title']} {a.get('summary', '')} {a.get('_fulltext', '')}".lower()
                if any(w in searchable for w in query_words):
                    matched.append({"title": a["title"], "url": a["url"], "summary": a.get("summary", "")})
                    if len(matched) >= limit:
                        break

        return matched

    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ okolica.net: %s", e)
        return []


def search_okolica_old(query: str, limit: int = None) -> list[dict]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –Ω–∞ okolica.net.
    –ü—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ‚Äî fallback –Ω–∞ sibokolica.ru.
    """
    articles = search_okolica_only(query, limit)
    if not articles:
        articles = _search_sibokolica(query, limit or ARTICLES_LIMIT_SEARCH)
    return articles


def _search_sibokolica(query: str, limit: int) -> list[dict]:
    """–ü–æ–∏—Å–∫ –Ω–∞ sibokolica.ru."""
    try:
        encoded = quote(query, safe="")
        url = f"{SITE_URL}/index.php?do=search&subaction=search&story={encoded}"
        response = _make_request(url)
        response.encoding = "utf-8"
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        articles = []

        for h2 in soup.find_all("h2"):
            link = h2.find("a")
            if not link or not link.get("href") or ".html" not in link.get("href", ""):
                continue

            href = link.get("href", "")
            if not href.startswith("http"):
                href = SITE_URL + (href if href.startswith("/") else "/" + href)

            title = h2.get_text(strip=True)
            summary = _extract_summary(h2, title, max_len=150)

            articles.append({"title": title, "url": href, "summary": summary})
            if len(articles) >= limit:
                break

        return articles

    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ sibokolica.ru: %s", e)
        return []


# WMO –∫–æ–¥—ã –ø–æ–≥–æ–¥—ã Open-Meteo ‚Üí –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
_WEATHER_CODE_RU = {
    0: "—è—Å–Ω–æ",
    1: "–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —è—Å–Ω–æ",
    2: "–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–±–ª–∞—á–Ω–æ—Å—Ç—å",
    3: "–ø–∞—Å–º—É—Ä–Ω–æ",
    45: "—Ç—É–º–∞–Ω",
    48: "–∏–∑–º–æ—Ä–æ–∑—å",
    51: "—Å–ª–∞–±–∞—è –º–æ—Ä–æ—Å—å",
    53: "–º–æ—Ä–æ—Å—å",
    55: "—Å–∏–ª—å–Ω–∞—è –º–æ—Ä–æ—Å—å",
    56: "—Å–ª–∞–±–∞—è –ª–µ–¥—è–Ω–∞—è –º–æ—Ä–æ—Å—å",
    57: "–ª–µ–¥—è–Ω–∞—è –º–æ—Ä–æ—Å—å",
    61: "–Ω–µ–±–æ–ª—å—à–æ–π –¥–æ–∂–¥—å",
    63: "–¥–æ–∂–¥—å",
    65: "—Å–∏–ª—å–Ω—ã–π –¥–æ–∂–¥—å",
    66: "—Å–ª–∞–±—ã–π –ª–µ–¥—è–Ω–æ–π –¥–æ–∂–¥—å",
    67: "–ª–µ–¥—è–Ω–æ–π –¥–æ–∂–¥—å",
    71: "–Ω–µ–±–æ–ª—å—à–æ–π —Å–Ω–µ–≥",
    73: "—Å–Ω–µ–≥",
    75: "—Å–∏–ª—å–Ω—ã–π —Å–Ω–µ–≥",
    77: "—Å–Ω–µ–∂–Ω–∞—è –∫—Ä—É–ø–∞",
    80: "–Ω–µ–±–æ–ª—å—à–æ–π –ª–∏–≤–µ–Ω—å",
    81: "–ª–∏–≤–µ–Ω—å",
    82: "—Å–∏–ª—å–Ω—ã–π –ª–∏–≤–µ–Ω—å",
    85: "—Å–Ω–µ–≥–æ–ø–∞–¥",
    86: "—Å–∏–ª—å–Ω—ã–π —Å–Ω–µ–≥–æ–ø–∞–¥",
    95: "–≥—Ä–æ–∑–∞",
    96: "–≥—Ä–æ–∑–∞ —Å –Ω–µ–±–æ–ª—å—à–∏–º –≥—Ä–∞–¥–æ–º",
    99: "–≥—Ä–æ–∑–∞ —Å –≥—Ä–∞–¥–æ–º",
}


def _weather_desc(code: int) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ WMO-–∫–æ–¥–∞ –≤ —Ç–µ–∫—Å—Ç."""
    return _WEATHER_CODE_RU.get(int(code), "–±–µ–∑ –æ—Å–∞–¥–∫–æ–≤")


def get_weather() -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–≥–æ–¥—ã —á–µ—Ä–µ–∑ Open-Meteo API (–±–µ—Å–ø–ª–∞—Ç–Ω–æ, –±–µ–∑ API-–∫–ª—é—á–∞)."""
    from config import WEATHER_CITY, WEATHER_LAT, WEATHER_LON, WEATHER_TIMEZONE

    try:
        url = "https://api.open-meteo.com/v1/forecast"
        response = requests.get(
            url,
            params={
                "latitude": WEATHER_LAT,
                "longitude": WEATHER_LON,
                "current": "temperature_2m,weather_code",
                "daily": "weather_code,temperature_2m_max,temperature_2m_min",
                "timezone": WEATHER_TIMEZONE,
                "forecast_days": 1,
            },
            timeout=REQUEST_TIMEOUT,
        )
        data = response.json()

        if "error" in data:
            logger.warning("Open-Meteo error: %s", data.get("reason", data))
            return "üå°Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–≥–æ–¥–µ"

        curr = data.get("current", {})
        daily = data.get("daily", {})

        temp = curr.get("temperature_2m")
        code = curr.get("weather_code", 0)
        desc = _weather_desc(code)

        parts = [f"üå°Ô∏è {WEATHER_CITY}: {temp:+.0f}¬∞C, {desc}"]

        times = daily.get("time", [])
        if times:
            t_max = daily.get("temperature_2m_max", [None])[0]
            t_min = daily.get("temperature_2m_min", [None])[0]
            if t_max is not None and t_min is not None:
                parts.append(f"–î–Ω—ë–º: {t_max:+.0f}¬∞C, –Ω–æ—á—å—é: {t_min:+.0f}¬∞C")

        return "\n".join(parts)

    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–≥–æ–¥—ã: %s", e)
        return "üå°Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ–≥–æ–¥—ã"
