#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""–ü–∞—Ä—Å–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å —Å–∞–π—Ç–æ–≤ –≥–∞–∑–µ—Ç—ã"""

import logging
import re
import time
import xml.etree.ElementTree as ET
from urllib.parse import quote

import requests
from bs4 import BeautifulSoup

from config import (
    SITE_URL,
    OLD_SITE_URL,
    USER_AGENT,
    REQUEST_TIMEOUT,
    ARTICLES_LIMIT_SEARCH,
    ARTICLES_LIMIT_ARCHIVE,
    OKOLICA_HTML_PAGES,
    OKOLICA_CATEGORY_PAGES,
    OKOLICA_GAZETA_PAGES,
    OKOLICA_GAZETA_PAGES_ARCHIVE,
)

# –†–∞–∑–¥–µ–ª—ã okolica.net –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ = –≥–ª–∞–≤–Ω–∞—è –ª–µ–Ω—Ç–∞)
_OKOLICA_CATEGORIES = ["", "rayon", "busines", "gorod", "foto"]

# –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–Ω–µ —É—á–∞—Å—Ç–≤—É—é—Ç –≤ –ø–æ–∏—Å–∫–µ)
_STOP_WORDS = frozenset({
    "–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–∏–∑", "–∫", "–æ—Ç", "–¥–ª—è", "–æ", "–æ–±", "–∞", "–Ω–æ", "—É",
    "–∂–µ", "–∏–ª–∏", "–∫–∞–∫", "—á—Ç–æ", "—ç—Ç–æ", "–≤—Å—ë", "–≤—Å–µ", "–µ–≥–æ", "–µ—ë", "–∏—Ö", "–æ–Ω", "–æ–Ω–∞",
    "–æ–Ω–∏", "–º—ã", "–≤—ã", "—è", "–Ω–µ", "–Ω–∏", "–±–µ–∑", "–¥–æ", "–∑–∞", "–ø—Ä–∏", "–ø—Ä–æ", "—Ç–∞–∫",
    "—É–∂–µ", "–µ—â–µ", "—Ç–æ–∂–µ", "—Ç–æ–ª—å–∫–æ", "–º–æ–∂–Ω–æ", "–±—ã—Ç—å", "–µ—Å—Ç—å", "–±—ã–ª", "–±—ã–ª–∞",
})

# –°–∏–Ω–æ–Ω–∏–º—ã: –∫–ª—é—á ‚Üí –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞
_SYNONYMS = {
    "–ø–æ—ç–∑–∏—è": ["—Å—Ç–∏—Ö–∏", "—Å—Ç–∏—Ö", "—Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ"],
    "—Å—Ç–∏—Ö–∏": ["–ø–æ—ç–∑–∏—è", "—Å—Ç–∏—Ö"],
    "—Å—Ç–∏—Ö": ["–ø–æ—ç–∑–∏—è", "—Å—Ç–∏—Ö–∏"],
    "—Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ": ["–ø–æ—ç–∑–∏—è", "—Å—Ç–∏—Ö–∏"],
    "—Ä–∞—Å—Å–∫–∞–∑": ["–∏—Å—Ç–æ—Ä–∏—è", "–æ—á–µ—Ä–∫"],
    "–æ—á–µ—Ä–∫": ["—Ä–∞—Å—Å–∫–∞–∑", "—Å—Ç–∞—Ç—å—è"],
    "—Å—Ç–∞—Ç—å—è": ["–æ—á–µ—Ä–∫", "–º–∞—Ç–µ—Ä–∏–∞–ª"],
    "–ø–æ–±–µ–¥–∞": ["–ø–æ–±–µ–¥–∏—Ç–µ–ª—å", "–ø–æ–±–µ–¥–Ω—ã–π"],
    "—à–∫–æ–ª–∞": ["—à–∫–æ–ª—å–Ω–∏–∫", "—à–∫–æ–ª—å–Ω—ã–π"],
    "–¥–µ—Ç–∏": ["—Ä–µ–±–µ–Ω–æ–∫", "—Ä–µ–±—è—Ç–∞"],
    "–ø—Ä–∞–∑–¥–Ω–∏–∫": ["–ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–π", "–ø—Ä–∞–∑–¥–Ω–æ–≤–∞–Ω–∏–µ"],
    "–≤–æ–π–Ω–∞": ["–≤–æ–µ–Ω–Ω—ã–π", "—Ñ—Ä–æ–Ω—Ç"],
    "—Ç–∞—Ç–∞—Ä—Å–∫": ["—Ç–∞—Ç–∞—Ä—Å–∫–∏–π"],
}

logger = logging.getLogger(__name__)

_morph_analyzer = None


def _get_morph():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞."""
    global _morph_analyzer
    if _morph_analyzer is None:
        try:
            import pymorphy2
            _morph_analyzer = pymorphy2.MorphAnalyzer()
        except ImportError:
            _morph_analyzer = False
    return _morph_analyzer


def _normalize_word(word: str) -> str:
    """–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ (–ª–µ–º–º–∞) –¥–ª—è –ø–æ–∏—Å–∫–∞."""
    morph = _get_morph()
    if not morph:
        return word.lower()
    try:
        parsed = morph.parse(word.lower())
        if parsed:
            return parsed[0].normal_form
    except Exception:
        pass
    return word.lower()


def _expand_with_synonyms(words: list[str]) -> list[str]:
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã –∫ —Å–ø–∏—Å–∫—É —Å–ª–æ–≤."""
    seen = set(words)
    result = list(words)
    for w in words:
        if w in _SYNONYMS:
            for s in _SYNONYMS[w]:
                if s not in seen and len(s) >= 2:
                    seen.add(s)
                    result.append(s)
        else:
            for key, syns in _SYNONYMS.items():
                if w in syns and key not in seen:
                    seen.add(key)
                    result.append(key)
                    break
    return result


def _extract_and_expand_query(query: str) -> list[str]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, —Å—Ç–µ–º–º–∏–Ω–≥, —Å–∏–Ω–æ–Ω–∏–º—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤).
    """
    words = [w for w in re.findall(r"[–∞-—è—ëa-z0-9]+", query.lower()) if len(w) >= 2]
    words = [w for w in words if w not in _STOP_WORDS]
    if not words:
        return []

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)
    normalized = []
    seen = set()
    for w in words:
        norm = _normalize_word(w)
        if norm not in seen:
            seen.add(norm)
            normalized.append(norm)

    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    return _expand_with_synonyms(normalized)


def _make_request(url: str, params: dict = None, retries: int = 2) -> requests.Response:
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP-–∑–∞–ø—Ä–æ—Å–∞ —Å –æ–±—â–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏ –ø–æ–≤—Ç–æ—Ä–æ–º –ø—Ä–∏ 5xx."""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
    }
    last_error = None
    for attempt in range(retries + 1):
        try:
            resp = requests.get(
                url,
                params=params,
                headers=headers,
                timeout=REQUEST_TIMEOUT,
            )
            if resp.status_code == 503 and attempt < retries:
                time.sleep(1.5 * (attempt + 1))
                continue
            return resp
        except requests.RequestException as e:
            last_error = e
            if attempt < retries:
                time.sleep(1.0)
    raise last_error


def get_latest_articles(limit: int = 10) -> list[dict]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π —Å sibokolica.ru."""
    try:
        response = _make_request(SITE_URL)
        response.encoding = "utf-8"
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        articles = []

        for h2 in soup.find_all("h2"):
            link = h2.find("a")
            if not link or not link.get("href") or ".html" not in link.get("href", ""):
                continue

            href = link.get("href", "")
            if not href.startswith("http"):
                href = SITE_URL + (href if href.startswith("/") else "/" + href)

            title = h2.get_text(strip=True)
            if not title:
                continue

            summary = _extract_summary(h2, title, max_len=200)

            articles.append({"title": title, "url": href, "summary": summary})

            if len(articles) >= limit:
                break

        return articles

    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ sibokolica.ru: %s", e)
        return []


def _extract_summary(anchor_element, exclude_text: str, max_len: int = 200) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤."""
    parent = anchor_element.parent
    for _ in range(5):
        if not parent:
            break
        for elem in parent.find_all(["p", "div"]):
            txt = elem.get_text(strip=True)
            if (
                txt
                and txt != exclude_text
                and 30 < len(txt) < 300
                and not txt.startswith("http")
                and elem != anchor_element
            ):
                return txt[:max_len] + ("..." if len(txt) > max_len else "")
        parent = parent.parent
    return ""


def _elem_text(el) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ XML-—ç–ª–µ–º–µ–Ω—Ç–∞ (–≤–∫–ª—é—á–∞—è CDATA)."""
    if el is None:
        return ""
    return "".join(el.itertext()).strip()


def _fetch_okolica_rss() -> list[dict]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ RSS okolica.net.
    RSS —Å–æ–¥–µ—Ä–∂–∏—Ç title, description, fulltext ‚Äî –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞.
    """
    try:
        url = f"{OLD_SITE_URL}/news/rss.xml"
        response = _make_request(url)
        response.raise_for_status()
        try:
            text = response.content.decode("cp1251")
        except UnicodeDecodeError:
            text = response.content.decode("utf-8", errors="replace")
        root = ET.fromstring(text)
        items = root.findall(".//item")

        articles = []
        for item in items:
            link_el = item.find("link")
            if link_el is None or not link_el.text or "/news/" not in link_el.text:
                continue

            url_str = link_el.text.strip()
            if not url_str.startswith("http"):
                url_str = f"{OLD_SITE_URL}{url_str}" if url_str.startswith("/") else url_str
            url_str = url_str.replace("http://", "https://", 1)

            title_el = item.find("title")
            title = _elem_text(title_el)
            if not title:
                continue

            desc_el = item.find("description")
            summary = _elem_text(desc_el)[:200] if desc_el is not None else ""

            full_el = item.find("fulltext")
            fulltext = _elem_text(full_el) if full_el is not None else ""

            articles.append({
                "title": title,
                "url": url_str,
                "summary": summary,
                "_fulltext": fulltext,
            })

        return articles

    except ET.ParseError as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS okolica.net: %s", e)
        return []
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RSS okolica.net: %s", e)
        return []


def _fetch_okolica_html_from_path(
    base_path: str, max_pages: int, seen_urls: set
) -> list[dict]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π —Å –æ–¥–Ω–æ–π —Å–µ–∫—Ü–∏–∏ (news, news/rayon, news/busines)."""
    articles = []
    path = f"{OLD_SITE_URL}/news/{base_path}" if base_path else f"{OLD_SITE_URL}/news"
    path = path.rstrip("/")

    for page in range(1, max_pages + 1):
        try:
            url = f"{path}/?page={page}" if page > 1 else f"{path}/"
            response = _make_request(url)
            response.raise_for_status()
            try:
                text = response.content.decode("cp1251")
            except UnicodeDecodeError:
                text = response.content.decode("utf-8", errors="replace")
            soup = BeautifulSoup(text, "html.parser")

            for a in soup.find_all("a", href=True):
                href = a.get("href", "")
                if "/news/" not in href or ".html" not in href or "rss" in href:
                    continue
                if "/top.html" in href or "/last.html" in href:
                    continue
                if not re.search(r"/news/[^/]+/\d+\.html", href):
                    continue
                if href in seen_urls:
                    continue

                full_url = OLD_SITE_URL + href if not href.startswith("http") else href
                title = a.get_text(strip=True)
                title = re.sub(r"\s*\[\.\.\.\]\s*$", "", title)
                if not title or len(title) < 5:
                    continue

                seen_urls.add(href)
                full_url_https = full_url.replace("http://", "https://", 1)
                articles.append({
                    "title": title,
                    "url": full_url_https,
                    "summary": "",
                    "_fulltext": "",
                })

        except Exception as e:
            logger.warning("–û—à–∏–±–∫–∞ HTML okolica.net %s page %s: %s", base_path or "news", page, e)
            break

    return articles


def _fetch_okolica_html(max_pages: int = None) -> list[dict]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π —Å–æ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ okolica.net: –≥–ª–∞–≤–Ω–∞—è –ª–µ–Ω—Ç–∞ + rayon, busines.
    –°–∞–π—Ç –≤ cp1251.
    """
    seen_urls: set = set()
    all_articles = []

    # –ì–ª–∞–≤–Ω–∞—è –ª–µ–Ω—Ç–∞ /news/
    all_articles.extend(
        _fetch_okolica_html_from_path("", OKOLICA_HTML_PAGES, seen_urls)
    )

    # –†–∞–∑–¥–µ–ª—ã rayon, busines
    for cat in _OKOLICA_CATEGORIES:
        if not cat:
            continue
        all_articles.extend(
            _fetch_okolica_html_from_path(cat, OKOLICA_CATEGORY_PAGES, seen_urls)
        )

    return all_articles


def _fetch_okolica_gazeta(max_pages: int = None) -> list[dict]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å—Ç–∞—Ç–µ–π –∏–∑ –∞—Ä—Ö–∏–≤–∞ –≥–∞–∑–µ—Ç—ã /gazeta/.
    –§–æ—Ä–º–∞—Ç: ¬´‚Ä¢ –ó–∞–≥–æ–ª–æ–≤–æ–∫ 1 ‚Ä¢ –ó–∞–≥–æ–ª–æ–≤–æ–∫ 2¬ª ‚Äî —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ ‚Ä¢ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –ø–æ–∏—Å–∫.
    URL –≤–µ–¥—ë—Ç –Ω–∞ –∞—Ä—Ö–∏–≤ (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–∞–π–¥—ë—Ç –≤—ã–ø—É—Å–∫).
    """
    max_pages = max_pages or OKOLICA_GAZETA_PAGES
    all_articles = []
    seen_titles: set = set()

    for page in range(1, max_pages + 1):
        try:
            url = f"{OLD_SITE_URL}/gazeta/" + (f"?page={page}" if page > 1 else "")
            response = _make_request(url)
            response.raise_for_status()
            try:
                text = response.content.decode("cp1251")
            except UnicodeDecodeError:
                text = response.content.decode("utf-8", errors="replace")
            soup = BeautifulSoup(text, "html.parser")

            # –ò—â–µ–º –±–ª–æ–∫–∏ —Å –≤—ã–ø—É—Å–∫–∞–º–∏: –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–∞—Ç–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ ¬´‚Ä¢ –¢–µ–∫—Å—Ç ‚Ä¢ –¢–µ–∫—Å—Ç¬ª
            for elem in soup.find_all(["p", "div", "li", "td"]):
                txt = elem.get_text(separator=" ", strip=True)
                if "‚Ä¢" not in txt or len(txt) < 10:
                    continue
                # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ ‚Ä¢ –∏ –±–µ—Ä—ë–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                parts = [p.strip() for p in txt.split("‚Ä¢") if len(p.strip()) >= 5]
                for title in parts:
                    title = re.sub(r"\s+", " ", title)
                    if len(title) < 5 or title in seen_titles:
                        continue
                    seen_titles.add(title)
                    all_articles.append({
                        "title": title,
                        "url": f"{OLD_SITE_URL}/gazeta/",
                        "summary": "",
                        "_fulltext": title,
                    })

        except Exception as e:
            logger.warning("–û—à–∏–±–∫–∞ gazeta okolica.net page %s: %s", page, e)
            break

    return all_articles


_lemma_cache: dict[str, str] = {}
_LEMMA_CACHE_MAX = 5000


def _get_lemma(word: str) -> str:
    """–õ–µ–º–º–∞ —Å–ª–æ–≤–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    w = word.lower()
    if w not in _lemma_cache:
        if len(_lemma_cache) >= _LEMMA_CACHE_MAX:
            _lemma_cache.clear()
        _lemma_cache[w] = _normalize_word(w)
    return _lemma_cache[w]


def _word_matches(query_word: str, searchable: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç. –£—á–∏—Ç—ã–≤–∞–µ—Ç:
    - —Ç–æ—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ (–¥–µ–Ω—å –≤ –¥–µ–Ω—å–≥–∏)
    - —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –ª–µ–º–º–µ (—à–∫–æ–ª–∞/—à–∫–æ–ª—ã/—à–∫–æ–ª—å–Ω–∏–∫ —á–µ—Ä–µ–∑ pymorphy2)
    - —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É 3 —Å–∏–º–≤–æ–ª–∞ (fallback)
    """
    searchable_lower = searchable.lower()
    if query_word in searchable_lower:
        return True

    query_lemma = _get_lemma(query_word)
    words_in_text = re.findall(r"[–∞-—è—ëa-z]{2,}", searchable_lower)

    for w in words_in_text:
        word_lemma = _get_lemma(w)
        if query_lemma == word_lemma:
            return True
        if query_word in word_lemma or word_lemma in query_word:
            return True
        # –ü—Ä–µ—Ñ–∏–∫—Å 3+ —Å–∏–º–≤–æ–ª–æ–≤
        if len(query_lemma) >= 3 and len(word_lemma) >= 3:
            if query_lemma[:3] == word_lemma[:3]:
                return True

    return False


def _count_matches(query_words: list[str], searchable: str) -> int:
    """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–≤—à–∏—Ö —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ (–¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏)."""
    return sum(1 for w in query_words if _word_matches(w, searchable))


def _run_search(articles: list[dict], query_words: list[str], limit: int) -> list[dict]:
    """
    –ü–æ–∏—Å–∫ –ø–æ —Å–ø–∏—Å–∫—É —Å—Ç–∞—Ç–µ–π —Å —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
    –°–Ω–∞—á–∞–ª–∞ ‚Äî –≤—Å–µ —Å–ª–æ–≤–∞, –∑–∞—Ç–µ–º ‚Äî –ª—é–±–æ–µ —Å–ª–æ–≤–æ. –í–Ω—É—Ç—Ä–∏ ‚Äî –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.
    """
    if not query_words:
        return []

    # –í—Å–µ —Å–ª–æ–≤–∞
    full_match = []
    for a in articles:
        searchable = f"{a['title']} {a.get('summary', '')} {a.get('_fulltext', '')}"
        if all(_word_matches(w, searchable) for w in query_words):
            cnt = _count_matches(query_words, searchable)
            full_match.append((cnt, a))

    if full_match:
        full_match.sort(key=lambda x: (-x[0], x[1]["title"]))
        return [
            {"title": a["title"], "url": a["url"], "summary": a.get("summary", "")}
            for _, a in full_match[:limit]
        ]

    # –õ—é–±–æ–µ —Å–ª–æ–≤–æ
    any_match = []
    for a in articles:
        searchable = f"{a['title']} {a.get('summary', '')} {a.get('_fulltext', '')}"
        cnt = _count_matches(query_words, searchable)
        if cnt > 0:
            any_match.append((cnt, a))

    any_match.sort(key=lambda x: (-x[0], x[1]["title"]))
    return [
        {"title": a["title"], "url": a["url"], "summary": a.get("summary", "")}
        for _, a in any_match[:limit]
    ]


def _merge_okolica_sources(
    rss_articles: list[dict], html_articles: list[dict], gazeta_articles: list[dict] = None
) -> list[dict]:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç RSS, HTML –∏ gazeta. –ü—Ä–∏ –¥—É–±–ª–∏–∫–∞—Ç–µ –ø–æ URL –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É RSS.
    Gazeta: –º–Ω–æ–≥–æ —Å—Ç–∞—Ç–µ–π —Å –æ–¥–Ω–∏–º URL, —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –ø–æ title.
    """
    by_url: dict[str, dict] = {}
    for a in rss_articles:
        url = a["url"].replace("http://", "https://", 1).rstrip("/")
        by_url[url] = a
    for a in html_articles:
        url = a["url"].replace("http://", "https://", 1).rstrip("/")
        if url not in by_url:
            by_url[url] = a
    result = list(by_url.values())
    result.extend(gazeta_articles or [])
    return result


def search_okolica_news(query: str, limit: int = None) -> list[dict]:
    """
    –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ okolica.net: RSS + —Ä–∞–∑–¥–µ–ª—ã –Ω–æ–≤–æ—Å—Ç–µ–π (rayon, busines, gorod, foto).
    –ë–µ–∑ –∞—Ä—Ö–∏–≤–∞ –≥–∞–∑–µ—Ç—ã.
    """
    limit = limit or ARTICLES_LIMIT_SEARCH
    try:
        rss_articles = _fetch_okolica_rss()
        html_articles = _fetch_okolica_html()
        articles = _merge_okolica_sources(rss_articles, html_articles, gazeta_articles=None)
        if not articles:
            return []
        query_words = _extract_and_expand_query(query)
        return _run_search(articles, query_words, limit)
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π okolica.net: %s", e)
        return []


def search_okolica_archive(query: str, limit: int = None) -> list[dict]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –∞—Ä—Ö–∏–≤—É –≥–∞–∑–µ—Ç—ã: –ø–æ—ç–∑–∏—è, —Ä–∞—Å—Å–∫–∞–∑—ã, –æ—á–µ—Ä–∫–∏ –∏ –¥—Ä.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ö–≤–∞—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –∞—Ä—Ö–∏–≤–∞.
    """
    limit = limit or ARTICLES_LIMIT_ARCHIVE
    try:
        gazeta_articles = _fetch_okolica_gazeta(OKOLICA_GAZETA_PAGES_ARCHIVE)
        if not gazeta_articles:
            return []
        query_words = _extract_and_expand_query(query)
        return _run_search(gazeta_articles, query_words, limit)
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∞—Ä—Ö–∏–≤—É okolica.net: %s", e)
        return []


def search_okolica_only(query: str, limit: int = None) -> list[dict]:
    """
    –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞ okolica.net (–Ω–æ–≤–æ—Å—Ç–∏ + –∞—Ä—Ö–∏–≤).
    –î–ª—è —Ä–∞–∑–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ search_okolica_news –∏ search_okolica_archive.
    """
    limit = limit or ARTICLES_LIMIT_SEARCH
    try:
        rss_articles = _fetch_okolica_rss()
        html_articles = _fetch_okolica_html()
        gazeta_articles = _fetch_okolica_gazeta()
        articles = _merge_okolica_sources(rss_articles, html_articles, gazeta_articles)
        if not articles:
            return []
        query_words = _extract_and_expand_query(query)
        return _run_search(articles, query_words, limit)
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ okolica.net: %s", e)
        return []


def search_okolica_old(query: str, limit: int = None) -> list[dict]:
    """
    –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –Ω–∞ okolica.net.
    –ü—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ‚Äî fallback –Ω–∞ sibokolica.ru.
    """
    articles = search_okolica_only(query, limit)
    if not articles:
        articles = _search_sibokolica(query, limit or ARTICLES_LIMIT_SEARCH)
    return articles


def _search_sibokolica(query: str, limit: int) -> list[dict]:
    """–ü–æ–∏—Å–∫ –Ω–∞ sibokolica.ru."""
    try:
        encoded = quote(query, safe="")
        url = f"{SITE_URL}/index.php?do=search&subaction=search&story={encoded}"
        response = _make_request(url)
        response.encoding = "utf-8"
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")
        articles = []

        for h2 in soup.find_all("h2"):
            link = h2.find("a")
            if not link or not link.get("href") or ".html" not in link.get("href", ""):
                continue

            href = link.get("href", "")
            if not href.startswith("http"):
                href = SITE_URL + (href if href.startswith("/") else "/" + href)

            title = h2.get_text(strip=True)
            summary = _extract_summary(h2, title, max_len=150)

            articles.append({"title": title, "url": href, "summary": summary})
            if len(articles) >= limit:
                break

        return articles

    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ sibokolica.ru: %s", e)
        return []


# WMO –∫–æ–¥—ã –ø–æ–≥–æ–¥—ã Open-Meteo ‚Üí –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
_WEATHER_CODE_RU = {
    0: "—è—Å–Ω–æ",
    1: "–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —è—Å–Ω–æ",
    2: "–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–±–ª–∞—á–Ω–æ—Å—Ç—å",
    3: "–ø–∞—Å–º—É—Ä–Ω–æ",
    45: "—Ç—É–º–∞–Ω",
    48: "–∏–∑–º–æ—Ä–æ–∑—å",
    51: "—Å–ª–∞–±–∞—è –º–æ—Ä–æ—Å—å",
    53: "–º–æ—Ä–æ—Å—å",
    55: "—Å–∏–ª—å–Ω–∞—è –º–æ—Ä–æ—Å—å",
    56: "—Å–ª–∞–±–∞—è –ª–µ–¥—è–Ω–∞—è –º–æ—Ä–æ—Å—å",
    57: "–ª–µ–¥—è–Ω–∞—è –º–æ—Ä–æ—Å—å",
    61: "–Ω–µ–±–æ–ª—å—à–æ–π –¥–æ–∂–¥—å",
    63: "–¥–æ–∂–¥—å",
    65: "—Å–∏–ª—å–Ω—ã–π –¥–æ–∂–¥—å",
    66: "—Å–ª–∞–±—ã–π –ª–µ–¥—è–Ω–æ–π –¥–æ–∂–¥—å",
    67: "–ª–µ–¥—è–Ω–æ–π –¥–æ–∂–¥—å",
    71: "–Ω–µ–±–æ–ª—å—à–æ–π —Å–Ω–µ–≥",
    73: "—Å–Ω–µ–≥",
    75: "—Å–∏–ª—å–Ω—ã–π —Å–Ω–µ–≥",
    77: "—Å–Ω–µ–∂–Ω–∞—è –∫—Ä—É–ø–∞",
    80: "–Ω–µ–±–æ–ª—å—à–æ–π –ª–∏–≤–µ–Ω—å",
    81: "–ª–∏–≤–µ–Ω—å",
    82: "—Å–∏–ª—å–Ω—ã–π –ª–∏–≤–µ–Ω—å",
    85: "—Å–Ω–µ–≥–æ–ø–∞–¥",
    86: "—Å–∏–ª—å–Ω—ã–π —Å–Ω–µ–≥–æ–ø–∞–¥",
    95: "–≥—Ä–æ–∑–∞",
    96: "–≥—Ä–æ–∑–∞ —Å –Ω–µ–±–æ–ª—å—à–∏–º –≥—Ä–∞–¥–æ–º",
    99: "–≥—Ä–æ–∑–∞ —Å –≥—Ä–∞–¥–æ–º",
}


def _weather_desc(code: int) -> str:
    """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ WMO-–∫–æ–¥–∞ –≤ —Ç–µ–∫—Å—Ç."""
    return _WEATHER_CODE_RU.get(int(code), "–±–µ–∑ –æ—Å–∞–¥–∫–æ–≤")


def get_weather() -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–≥–æ–¥—ã —á–µ—Ä–µ–∑ Open-Meteo API (–±–µ—Å–ø–ª–∞—Ç–Ω–æ, –±–µ–∑ API-–∫–ª—é—á–∞)."""
    from config import WEATHER_CITY, WEATHER_LAT, WEATHER_LON, WEATHER_TIMEZONE

    try:
        url = "https://api.open-meteo.com/v1/forecast"
        response = requests.get(
            url,
            params={
                "latitude": WEATHER_LAT,
                "longitude": WEATHER_LON,
                "current": "temperature_2m,weather_code",
                "daily": "weather_code,temperature_2m_max,temperature_2m_min",
                "timezone": WEATHER_TIMEZONE,
                "forecast_days": 1,
            },
            timeout=REQUEST_TIMEOUT,
        )
        data = response.json()

        if "error" in data:
            logger.warning("Open-Meteo error: %s", data.get("reason", data))
            return "üå°Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–≥–æ–¥–µ"

        curr = data.get("current", {})
        daily = data.get("daily", {})

        temp = curr.get("temperature_2m")
        code = curr.get("weather_code", 0)
        desc = _weather_desc(code)

        parts = [f"üå°Ô∏è {WEATHER_CITY}: {temp:+.0f}¬∞C, {desc}"]

        times = daily.get("time", [])
        if times:
            t_max = daily.get("temperature_2m_max", [None])[0]
            t_min = daily.get("temperature_2m_min", [None])[0]
            if t_max is not None and t_min is not None:
                parts.append(f"–î–Ω—ë–º: {t_max:+.0f}¬∞C, –Ω–æ—á—å—é: {t_min:+.0f}¬∞C")

        return "\n".join(parts)

    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–≥–æ–¥—ã: %s", e)
        return "üå°Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ–≥–æ–¥—ã"
